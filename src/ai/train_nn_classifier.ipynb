{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from random import randint\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, confusion_matrix, roc_auc_score, recall_score, precision_score\n",
    "\n",
    "# own Modules \n",
    "from nn_model import NeuralNetwork\n",
    "from data_set import DataSetLoans\n",
    "from trainer import Trainer\n",
    "from logger import Logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"path\" : '../../data/reduced_dataset.csv' ,\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"input_size\" : 125,\n",
    "        \"batch_size\" : 8,\n",
    "        \"dropout_rate_fc\": 0.2,\n",
    "        \"units_fc1\": 60,\n",
    "        \"units_fc2\": 30,\n",
    "        \"units_fc3\": 5,\n",
    "    },\n",
    "    \"cycling_lr\" : {\n",
    "        \"scheduler_active\" : True, \n",
    "        # Mode can be one of {triangular, triangular2, exp_range}\n",
    "        \"mode\" : \"triangular\", \n",
    "        \"gamma\" : 0.9995,\n",
    "        \"base_lr\" : 0.001, \n",
    "        \"max_lr\" :0.05\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"n_epochs\" : 20, # Mit mehr Rechenpower sollte die Anazahl an epochen deutlich höher sein!\n",
    "        \"patience\" : 10,\n",
    "    },\n",
    "    \"filed_location\": {\n",
    "        \"trained_model\" : \"./models/trained_model/\",\n",
    "        \"log_file\" : \"./models/log/\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(421096, 76)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set = pd.read_csv(\"../../data/reduced_dataset.csv\")\n",
    "data_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(326529, 76)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set = data_set.dropna()\n",
    "data_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Target Variable and Delet Samples with Status \"Current\"\n",
    "Bei allen Loans mit dem Status Current ist nicht klar, ob diese zu einem guten oder schlechten Loan entwicklen. Deshalb lösche ich diese Samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166731, 76)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_target_variable(attribute):\n",
    "    if attribute == 'Fully Paid' or attribute == 'In Grace Period':\n",
    "        return 0\n",
    "    elif attribute == 'Current':\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "data_set['target'] = data_set['loan_status'].apply(create_target_variable)\n",
    "data_set = data_set[data_set['target'] != -1]\n",
    "data_set.drop(labels='loan_status', axis=1, inplace=True)\n",
    "data_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Purpose Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_purpose_attributes(attribute):\n",
    "    if attribute=='debt_consolidation' or attribute=='credit_card':\n",
    "        return 'refinance'\n",
    "    elif attribute=='house' or attribute=='home_improvement' or attribute=='moving':\n",
    "        return 'house'\n",
    "    elif attribute=='car' or attribute=='major_purchase':\n",
    "        return 'major_purchase'\n",
    "    else:\n",
    "        return 'other'\n",
    "data_set['purpose'] = data_set['purpose'].apply(new_purpose_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Grade Feature\n",
    "Ich schließe die Noten des Lending Clubs  aus. Sie werden wahrscheinlich mit Hilfe eines Clustering-Algorithmus generiert und würde einen künstlichen Einblick in die Kreditnehmer gewähren (Information Leakage) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166731, 75)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.drop(labels='grade', axis=1, inplace=True)\n",
    "data_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train Data Set and Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_set['target']\n",
    "X = data_set.drop(['target'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X).to_numpy()\n",
    "y = pd.get_dummies(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421088</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421089</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421092</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421093</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421094</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>166731 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0  1\n",
       "4       1  0\n",
       "5       0  1\n",
       "7       1  0\n",
       "9       1  0\n",
       "10      1  0\n",
       "...    .. ..\n",
       "421088  0  1\n",
       "421089  1  0\n",
       "421092  0  1\n",
       "421093  0  1\n",
       "421094  1  0\n",
       "\n",
       "[166731 rows x 2 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good Loans are represendet by the first column, bad loands by the second column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Test and Train Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_total, X_test, y_train_total, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train Data Set in Train and Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train_total, y_train_total, test_size=0.2, random_state=1, stratify=y_train_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "# Transform data\n",
    "train_scaled = scaler.transform(X_train)\n",
    "val_scaled = scaler.transform(X_val)\n",
    "test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Training\n",
    "### Create Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = DataSetLoans(train_scaled, y_train)\n",
    "dataset_val = DataSetLoans(val_scaled, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_training = DataLoader(dataset_train, \n",
    "                                  batch_size=param[\"model\"][\"batch_size\"], \n",
    "                                  num_workers=0, \n",
    "                                  shuffle=True, \n",
    "                                  drop_last=True\n",
    "                                 )\n",
    "data_loader_val = DataLoader(dataset_val, \n",
    "                             batch_size=param[\"model\"][\"batch_size\"], \n",
    "                             num_workers=0, \n",
    "                             shuffle=True, \n",
    "                             drop_last=True\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data of batch: 0\n",
      "Size of input data: torch.Size([8, 125])\n",
      "Size of target data: torch.Size([8, 2])\n",
      "Data of batch: 1\n",
      "Size of input data: torch.Size([8, 125])\n",
      "Size of target data: torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, data in enumerate(data_loader_training):\n",
    "    x,y = data\n",
    "    print('Data of batch: {}'.format(batch_idx))\n",
    "    print(\"Size of input data: {}\".format(x.size()))\n",
    "    print(\"Size of target data: {}\".format(y.size()))\n",
    "    if batch_idx >=1: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "model = NeuralNetwork(batch_size=param['model']['batch_size'], \n",
    "                      input_dim=param['model']['input_size'], \n",
    "                      units_fc1=param['model']['units_fc1'], \n",
    "                      units_fc2=param['model']['units_fc1'], \n",
    "                      units_fc3=param['model']['units_fc1'], \n",
    "                      dropout_rate_fc= param['model']['dropout_rate_fc'],\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Binary Cross Entropy Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Optimizer and Cyclic Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1.)  \n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer=optimizer, \n",
    "                                              base_lr=param['cycling_lr']['base_lr'], \n",
    "                                              max_lr=param['cycling_lr']['max_lr'], \n",
    "                                              step_size_up=(len(X_train)/param['model']['batch_size'])*2, # Authors of Cyclic LR suggest setting step_size 2-8 x training iterations in epoch.\n",
    "                                              mode=param['cycling_lr']['mode'],\n",
    "                                              gamma=param['cycling_lr']['gamma']\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  scheduler=scheduler,\n",
    "                  scheduler_active = param[\"cycling_lr\"][\"scheduler_active\"],\n",
    "                  criterion=criterion, \n",
    "                  location_model=param[\"filed_location\"][\"trained_model\"], \n",
    "                  patience=param['training']['patience']\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Training with Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training phase is started\n",
      "Epoch 0: best model saved with loss: 0.0646147196813122\n",
      "Epoch 2: best model saved with loss: 0.06451253936447106\n",
      "Epoch 3: best model saved with loss: 0.06135131168531977\n",
      "Epoch 4: best model saved with loss: 0.06075552679137766\n",
      "Epoch 6: best model saved with loss: 0.060310679740110576\n",
      "Epoch 7: best model saved with loss: 0.059257919152598044\n",
      "Epoch 8: best model saved with loss: 0.058897964040577594\n",
      "Epoch 9: best model saved with loss: 0.05875155231882986\n",
      "Epoch 10: best model saved with loss: 0.05852654331776725\n",
      "Epoch 11: best model saved with loss: 0.05784867741711423\n",
      "Epoch 12: best model saved with loss: 0.057488227890817395\n",
      "Epoch 13: best model saved with loss: 0.05747834583686379\n",
      "Epoch 15: best model saved with loss: 0.057189733104195985\n",
      "Epoch 16: best model saved with loss: 0.05691413197582714\n",
      "Epoch 17: best model saved with loss: 0.05688596595484936\n",
      "Epoch 18: best model saved with loss: 0.0568499493152024\n",
      "Epoch 19: best model saved with loss: 0.05658452352987257\n",
      "Training phase is finished\n"
     ]
    }
   ],
   "source": [
    "# Initialise Logger\n",
    "session_id = str(randint(10000, 99999))\n",
    "logger = Logger(param[\"filed_location\"][\"log_file\"], session_id)\n",
    "\n",
    "# Log model architecture and training configuration\n",
    "logger.log_message(\"Architecture and Training configuration:\")\n",
    "logger.log_message(\"Batch size: {}\".format(param['model']['batch_size']))\n",
    "logger.log_message(\"Input size: {}\".format(param['model']['input_size']))\n",
    "logger.log_message(\"Number of Layers: {}\".format(3))\n",
    "logger.log_message(\"Units Layer 1: {}\".format(param['model']['units_fc1']))\n",
    "logger.log_message(\"Units Layer 2: {}\".format(param['model']['units_fc2']))\n",
    "logger.log_message(\"Units Layer 3: {}\".format(param['model']['units_fc3']))\n",
    "\n",
    "logger.log_message(\"Dropout rate fc NN: {}\".format(param['model']['dropout_rate_fc']))\n",
    "logger.log_message(\"Cycling LR mode: {}\".format(param['cycling_lr']['mode']))\n",
    "logger.log_message(\"Cycling LR base LR: {}\".format(param['cycling_lr']['base_lr']))\n",
    "logger.log_message(\"Cycling LR max LR: {}\".format(param['cycling_lr']['max_lr']))\n",
    "logger.log_message(\"- -\"*20)\n",
    "\n",
    "print(\"Training phase is started\")\n",
    "logger.log_message(\"Training phase is started\")\n",
    "torch.manual_seed(0)\n",
    "\n",
    "for epoch in range(param['training']['n_epochs']):\n",
    "    # Train with batches \n",
    "    mean_epoch_training_loss = trainer.train(data_loader_training)\n",
    "    mean_epoch_test_loss = trainer.evaluate(data_loader_val)\n",
    "\n",
    "    # Save model if its the best one since the last change in configuration of hyperparameters\n",
    "    status_ok = trainer.save_model(epoch, mean_epoch_test_loss, session_id)\n",
    "    \n",
    "    # Log information of current epoch\n",
    "    logger.log_current_statistics(epoch, mean_epoch_test_loss)\n",
    "    \n",
    "    if not status_ok:\n",
    "        break\n",
    "\n",
    "print(\"Training phase is finished\")\n",
    "logger.log_message(\"Training phase is finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions from Test Set\n",
    "Das Modell mit folgender Konfiguration erzielte den geringsten Loss: <br>\n",
    "Number of Layers: 3 <br>\n",
    "Units Layer 1: 60 <br>\n",
    "Units Layer 2: 30 <br>\n",
    "Units Layer 3: 5 <br>\n",
    "\n",
    "Daher wird hier das Modell mit der ID 77463 geladen\n",
    "### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(param[\"filed_location\"][\"trained_model\"]+\"id\"+str(77463))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = DataSetLoans(test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_test = DataLoader(dataset_test, \n",
    "                              batch_size=param[\"model\"][\"batch_size\"], \n",
    "                              num_workers=0, \n",
    "                              shuffle=False, \n",
    "                              drop_last=True\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = []\n",
    "for batch_number, data in enumerate(data_loader_test):\n",
    "    with torch.no_grad():\n",
    "        input_data, target_data = data\n",
    "        model.eval()\n",
    "        output_mini_batch = model(input_data)\n",
    "        for prediction_single_sample in output_mini_batch:\n",
    "            if prediction_single_sample[0] > prediction_single_sample[1]:\n",
    "                y_hat.append(0)\n",
    "            else:\n",
    "                y_hat.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An dieser Stelle könnte man noch mit verschiedenen Thresholds arbeiten, ab der die Prediction einer gewissen Klasse zugeordnet wird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revert Zero Hot Encoding\n",
    "y_test = [0 if i[0]==1 else 1 for i in y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1 Score is: 0.952204395308076\n",
      "The Precision Score is: 0.922720146328717\n",
      "The Recall Score is: 0.9836350974930362\n",
      "TN: 34473\n",
      "FP: 1183\n",
      "FN: 235\n",
      "TP: 14125\n"
     ]
    }
   ],
   "source": [
    "# because of the mini-batches, 4 elements\n",
    "y_test = y_test[:len(y_test)-4]\n",
    "# Evaluate Test Set\n",
    "print(\"The F1 Score is: {}\".format(f1_score(y_test, y_hat, average=\"binary\")))\n",
    "print(\"The Precision Score is: {}\".format(precision_score(y_test, y_hat, average=\"binary\")))\n",
    "print(\"The Recall Score is: {}\".format(recall_score(y_test, y_hat, average=\"binary\")))\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_hat).ravel()\n",
    "print(\"TN: {}\".format(tn))\n",
    "print(\"FP: {}\".format(fp))\n",
    "print(\"FN: {}\".format(fn))\n",
    "print(\"TP: {}\".format(tp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ginkgo_loan_project",
   "language": "python",
   "name": "ginkgo_loan_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
