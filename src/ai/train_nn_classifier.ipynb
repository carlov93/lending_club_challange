{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from random import randint\n",
    "import os\n",
    "\n",
    "# own Modules \n",
    "from data_preperator import DataPreperator\n",
    "from nn_model import NeuralNetwork\n",
    "from trainer import Trainer\n",
    "from logger import Logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Aus zeitlichen Gründen habe ich hier kein Cross Validation implementiert, sondern nur ein Training Datensatz und ein Test Datensatz.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"data\" : {\n",
    "        \"path\" : '../../data/reduced_dataset.csv' ,\n",
    "    },\n",
    "    \"model\" : {\n",
    "        \"input_size\" : 12,\n",
    "        \"batch_size\" : 8,\n",
    "        \"dropout_rate_fc\": 0.2,\n",
    "        \"units_fc1\": 40,\n",
    "        \"units_fc2\": 20\n",
    "        \"units_fc3\": 10,\n",
    "        \"units_fc4\": 5,\n",
    "    },\n",
    "    \"cycling_lr\" : {\n",
    "        \"scheduler_active\" : True, \n",
    "        # Mode can be one of {triangular, triangular2, exp_range}\n",
    "        \"mode\" : \"triangular\", \n",
    "        \"gamma\" : 0.9995,\n",
    "        \"base_lr\" : 0.001, \n",
    "        \"max_lr\" :0.05\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"n_epochs\" : 50,\n",
    "        \"patience\" : 10,\n",
    "    },\n",
    "    \"filed_location\": {\n",
    "        \"trained_model\" : \"./models/trained_model/\",\n",
    "        \"log_file\" : \"./models/log/\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(421096, 76)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set = pd.read_csv(\"../../data/reduced_dataset.csv\")\n",
    "data_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(326529, 76)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set = data_set.dropna()\n",
    "data_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Target Variable and Delet Samples with Status \"Current\"\n",
    "Bei allen Loans mit dem Status Current ist nicht klar, ob diese zu einem guten oder schlechten Loan entwicklen. Deshalb lösche ich diese Samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166731, 76)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_target_variable(attribute):\n",
    "    if attribute == 'Fully Paid' or attribute == 'In Grace Period':\n",
    "        return 0\n",
    "    elif attribute == 'Current':\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "data_set['target'] = data_set['loan_status'].apply(create_target_variable)\n",
    "data_set = data_set[data_set['target'] != -1]\n",
    "data_set.drop(labels='loan_status', axis=1, inplace=True)\n",
    "data_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Purpose Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_purpose_attributes(attribute):\n",
    "    if attribute=='debt_consolidation' or attribute=='credit_card':\n",
    "        return 'refinance'\n",
    "    elif attribute=='house' or attribute=='home_improvement' or attribute=='moving':\n",
    "        return 'house'\n",
    "    elif attribute=='car' or attribute=='major_purchase':\n",
    "        return 'major_purchase'\n",
    "    else:\n",
    "        return 'other'\n",
    "data_set['purpose'] = data_set['purpose'].apply(new_purpose_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Grade Feature\n",
    "Ich schließe die Noten des Lending Clubs  aus. Sie werden wahrscheinlich mit Hilfe eines Clustering-Algorithmus generiert und würde einen künstlichen Einblick in die Kreditnehmer gewähren (Information Leakage) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166731, 75)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.drop(labels='grade', axis=1, inplace=True)\n",
    "data_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Hot Encoding for Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = pd.get_dummies(data_set)\n",
    "data_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23611\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataPreperator(dataset=data_set, \n",
    "                              ignored_features=[],\n",
    "                              stake_training_data=0.8,\n",
    "                              features_not_to_scale=[])\n",
    "train_data, validation_data = train_loader.prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_training = DataLoader(train_data, \n",
    "                                  batch_size=param[\"model\"][\"batch_size\"], \n",
    "                                  num_workers=0, \n",
    "                                  shuffle=True, \n",
    "                                  drop_last=True\n",
    "                                 )\n",
    "data_loader_validation = DataLoader(validation_data, \n",
    "                                  batch_size=param[\"model\"][\"batch_size\"], \n",
    "                                  num_workers=0, \n",
    "                                  shuffle=True, \n",
    "                                  drop_last=True\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data of batch: 0\n",
      "Size of input data: torch.Size([8, 25, 2])\n",
      "Size of target data: torch.Size([8, 2])\n",
      "Data of batch: 1\n",
      "Size of input data: torch.Size([8, 25, 2])\n",
      "Size of target data: torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, data in enumerate(data_loader_training):\n",
    "    x,y = data\n",
    "    print('Data of batch: {}'.format(batch_idx))\n",
    "    print(\"Size of input data: {}\".format(x.size()))\n",
    "    print(\"Size of target data: {}\".format(y.size()))\n",
    "    if batch_idx >=1: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "model = NeuralNetwork(batch_size=param['model']['batch_size'], \n",
    "                      input_dim=param['model']['input_size'], \n",
    "                      units_fc1=param['model']['units_fc1'], \n",
    "                      units_fc2=param['model']['units_fc1'], \n",
    "                      units_fc3=param['model']['units_fc1'], \n",
    "                      units_fc4=param['model']['units_fc1'], \n",
    "                      dropout_rate_fc= param['model']['dropout_rate_fc'],\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define MSE Loss function as torch module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Optimizer and Cyclic Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1.)  \n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer=optimizer, \n",
    "                                              base_lr=param['cycling_lr']['base_lr'], \n",
    "                                              max_lr=param['cycling_lr']['max_lr'], \n",
    "                                              step_size_up=(len(train_data)/param['model']['batch_size'])*2, # Authors of Cyclic LR suggest setting step_size 2-8 x training iterations in epoch.\n",
    "                                              mode=param['cycling_lr']['mode'],\n",
    "                                              gamma=param['cycling_lr']['gamma']\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  scheduler=scheduler,\n",
    "                  scheduler_active = param[\"cycling_lr\"][\"scheduler_active\"],\n",
    "                  criterion=criterion, \n",
    "                  location_model=param[\"filed_location\"][\"trained_model\"], \n",
    "                  patience=param['training']['patience']\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training phase is started\n",
      "Epoch 0: best model saved with loss: 0.16020687000072864\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-882d24fda972>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Train with batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mmean_epoch_training_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Save model if its the best one since the last change in configuration of hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_notebooks/masterarbeit/src/ai/utils/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_loader_training)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_notebooks/masterarbeit/venv_pm/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_notebooks/masterarbeit/venv_pm/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialise Logger\n",
    "session_id = str(randint(10000, 99999))\n",
    "logger = Logger(param[\"filed_location\"][\"log_file\"], session_id)\n",
    "\n",
    "# Log model architecture and training configuration\n",
    "logger.log_message(\"Architecture and Training configuration:\")\n",
    "logger.log_message(\"Batch size: {}\".format(param['model']['batch_size']))\n",
    "logger.log_message(\"Input size: {}\".format(param['model']['input_size']))\n",
    "\n",
    "logger.log_message(\"Dropout rate fc NN: {}\".format(param['model']['dropout_rate_fc']))\n",
    "logger.log_message(\"Cycling LR mode: {}\".format(param['cycling_lr']['mode']))\n",
    "logger.log_message(\"Cycling LR base LR: {}\".format(param['cycling_lr']['base_lr']))\n",
    "logger.log_message(\"Cycling LR max LR: {}\".format(param['cycling_lr']['max_lr']))\n",
    "logger.log_message(\"- -\"*20)\n",
    "\n",
    "print(\"Training phase is started\")\n",
    "logger.log_message(\"Training phase is started\")\n",
    "torch.manual_seed(0)\n",
    "\n",
    "for epoch in range(param['training']['n_epochs']):\n",
    "    # Train with batches \n",
    "    mean_epoch_training_loss = trainer.train(data_loader_training)\n",
    "    mean_epoch_validation_loss = trainer.evaluate(data_loader_training)\n",
    "\n",
    "    # Save model if its the best one since the last change in configuration of hyperparameters\n",
    "    status_ok = trainer.save_model(epoch, mean_epoch_validation_loss, session_id)\n",
    "    \n",
    "    # Log information of current epoch\n",
    "    logger.log_current_statistics(epoch, mean_epoch_training_loss)\n",
    "    \n",
    "    if not status_ok:\n",
    "        break\n",
    "\n",
    "print(\"Training phase is finished\")\n",
    "logger.log_message(\"Training phase is finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterarbeit",
   "language": "python",
   "name": "masterarbeit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
